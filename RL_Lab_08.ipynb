{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmjkl2BKNkmTRb7z1cNDTU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavijulapelly/Reinforcement-Learning/blob/main/RL_Lab_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O89M1TcwEchl",
        "outputId": "0fab582a-45fe-4f4b-f759-5bd0fab53740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 10\tAvgReward(50)=-704.79\tLastReward=-588.47\tPolicyLoss=33.3017\tValueLoss=748.8160\tEntropy=0.9336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 20\tAvgReward(50)=-656.62\tLastReward=-534.66\tPolicyLoss=26.7379\tValueLoss=1161.5837\tEntropy=0.9358\n",
            "Ep 30\tAvgReward(50)=-679.25\tLastReward=-484.70\tPolicyLoss=137.9226\tValueLoss=11563.3984\tEntropy=0.9461\n",
            "Ep 40\tAvgReward(50)=-697.97\tLastReward=-763.56\tPolicyLoss=230.4587\tValueLoss=31512.4062\tEntropy=0.9535\n",
            "Ep 50\tAvgReward(50)=-703.34\tLastReward=-788.49\tPolicyLoss=320.4993\tValueLoss=85216.2031\tEntropy=0.9594\n",
            "Ep 60\tAvgReward(50)=-715.46\tLastReward=-784.14\tPolicyLoss=559.5554\tValueLoss=218181.1562\tEntropy=0.9626\n",
            "Done in 18.598207473754883\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "from collections import deque, namedtuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "try:\n",
        "    import gym\n",
        "except Exception as e:\n",
        "    raise ImportError(\"This script needs gym. Install with: pip install gym\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64,64), log_std_init=-0.5):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last = h\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.mean = nn.Linear(last, act_dim)\n",
        "        self.log_std = nn.Parameter(torch.ones(act_dim) * log_std_init)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        x = self.net(obs)\n",
        "        mean = self.mean(x)\n",
        "        log_std = self.log_std.expand_as(mean)\n",
        "        std = torch.exp(log_std)\n",
        "        return mean, std, log_std\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, hidden_sizes=(64,64)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, 1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.net(obs).squeeze(-1)\n",
        "\n",
        "Transition = namedtuple('Transition', ['obs', 'act', 'rew', 'next_obs', 'done', 'logp'])\n",
        "\n",
        "def gaussian_log_prob(mean, log_std, act):\n",
        "    var = torch.exp(2 * log_std)\n",
        "    logp = -0.5 * (((act - mean) ** 2) / var + 2 * log_std + math.log(2 * math.pi))\n",
        "    return logp.sum(axis=-1)\n",
        "\n",
        "def sample_action(mean, std):\n",
        "    eps = torch.randn_like(mean)\n",
        "    return mean + eps * std\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, obs_dim, act_dim, device='cpu', actor_lr=3e-4, critic_lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=1e-3, max_grad_norm=0.5):\n",
        "        self.device = device\n",
        "        self.actor = Actor(obs_dim, act_dim).to(device)\n",
        "        self.critic = Critic(obs_dim).to(device)\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.value_coef = value_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            mean, std, log_std = self.actor(obs_t)\n",
        "            act = sample_action(mean, std)\n",
        "            logp = gaussian_log_prob(mean, log_std, act)\n",
        "            value = self.critic(obs_t)\n",
        "        return act.cpu().numpy()[0], logp.cpu().numpy()[0], value.cpu().numpy()[0]\n",
        "\n",
        "    def update(self, transitions, last_value=0.0):\n",
        "        obs = torch.as_tensor(np.vstack([t.obs for t in transitions]), dtype=torch.float32, device=self.device)\n",
        "        acts = torch.as_tensor(np.vstack([t.act for t in transitions]), dtype=torch.float32, device=self.device)\n",
        "        rewards = [t.rew for t in transitions]\n",
        "        dones = [t.done for t in transitions]\n",
        "        old_logps = torch.as_tensor(np.array([t.logp for t in transitions]), dtype=torch.float32, device=self.device)\n",
        "        returns = []\n",
        "        R = last_value\n",
        "        for r, done in zip(reversed(rewards), reversed(dones)):\n",
        "            R = r + self.gamma * R * (1.0 - float(done))\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.as_tensor(returns, dtype=torch.float32, device=self.device)\n",
        "        values = self.critic(obs)\n",
        "        mean, std, log_std = self.actor(obs)\n",
        "        new_logps = gaussian_log_prob(mean, log_std, acts)\n",
        "        entropy = (0.5 * (1.0 + math.log(2 * math.pi)) + log_std).sum(-1).mean()\n",
        "        advantages = returns - values.detach()\n",
        "        policy_loss = -(new_logps * advantages).mean()\n",
        "        value_loss = 0.5 * (returns - values).pow(2).mean()\n",
        "        actor_loss = policy_loss - self.entropy_coef * entropy\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_actor.step()\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        value_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
        "        self.optimizer_critic.step()\n",
        "        return dict(policy_loss=policy_loss.item(), value_loss=value_loss.item(), entropy=entropy.item())\n",
        "\n",
        "def train(env_name='Pendulum-v1', seed=1, episodes=500, max_steps=200, n_steps=5, render=False):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    env = gym.make(env_name)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    agent = A2CAgent(obs_dim, act_dim, device=device)\n",
        "    total_rewards = []\n",
        "    for ep in range(1, episodes + 1):\n",
        "        obs = env.reset()\n",
        "        if isinstance(obs, tuple):\n",
        "            obs = obs[0]\n",
        "        ep_reward = 0.0\n",
        "        done = False\n",
        "        step = 0\n",
        "        buffer = []\n",
        "        while not done and step < max_steps:\n",
        "            act, logp, value = agent.get_action(obs)\n",
        "            next_obs, rew, terminated, truncated, info = None, None, None, None, None\n",
        "            try:\n",
        "                next_obs, rew, terminated, truncated, info = env.step(act)\n",
        "                done_flag = terminated or truncated\n",
        "            except Exception:\n",
        "                next_obs, rew, done_flag, info = env.step(act)\n",
        "            if isinstance(next_obs, tuple):\n",
        "                next_obs = next_obs[0]\n",
        "            buffer.append(Transition(obs=obs, act=act, rew=rew, next_obs=next_obs, done=done_flag, logp=logp))\n",
        "            ep_reward += rew\n",
        "            obs = next_obs\n",
        "            step += 1\n",
        "            if len(buffer) >= n_steps or done_flag or step >= max_steps:\n",
        "                if done_flag:\n",
        "                    last_value = 0.0\n",
        "                else:\n",
        "                    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                    with torch.no_grad():\n",
        "                        last_value = agent.critic(obs_t).cpu().numpy()[0]\n",
        "                stats = agent.update(buffer, last_value=last_value)\n",
        "                buffer = []\n",
        "            if done_flag:\n",
        "                break\n",
        "        total_rewards.append(ep_reward)\n",
        "        if ep % 10 == 0:\n",
        "            avg = np.mean(total_rewards[-50:])\n",
        "            print(f\"Ep {ep}\\tAvgReward(50)={avg:.2f}\\tLastReward={ep_reward:.2f}\\tPolicyLoss={stats['policy_loss']:.4f}\\tValueLoss={stats['value_loss']:.4f}\\tEntropy={stats['entropy']:.4f}\")\n",
        "    env.close()\n",
        "    return agent, total_rewards\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = time.time()\n",
        "    agent, rewards = train(episodes=60, max_steps=200, n_steps=5)\n",
        "    print('Done in', time.time() - start)"
      ]
    }
  ]
}