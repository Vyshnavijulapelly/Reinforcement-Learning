{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhGLq5gcIlrr6IOUrKR+8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavijulapelly/Reinforcement-Learning/blob/main/RL_Lab_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return (\n",
        "            np.stack(state),\n",
        "            np.array(action),\n",
        "            np.array(reward, dtype=np.float32),\n",
        "            np.stack(next_state),\n",
        "            np.array(done, dtype=np.uint8),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# Q-networks\n",
        "class DQN_MLP(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, act_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class DQN_CNN(nn.Module):\n",
        "    def __init__(self, input_shape, act_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_shape\n",
        "        assert h == 84 and w == 84, \"CNN expects 84x84 input\"\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, act_dim),\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / 255.0\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "\n",
        "# Atari preprocessing wrappers\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "        self._obs_buffer = np.zeros(\n",
        "            (2,) + env.observation_space.shape, dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "        return max_frame, total_reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class FrameProcessor(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        return np.expand_dims(obs, -1)\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_obs(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_obs(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.concatenate(list(self.frames), axis=-1)\n",
        "\n",
        "\n",
        "# Environment builder\n",
        "def make_env(env_id: str, seed: int, cnn: bool, frameskip: int, frame_stack: int):\n",
        "    env = gym.make(env_id, frameskip=1) if \"ALE/\" in env_id else gym.make(env_id)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    if cnn:\n",
        "        if \"ALE/\" not in env_id:\n",
        "            raise ValueError(\"--cnn was set but env is not Atari. Use ALE/Breakout-v5 etc.\")\n",
        "        env = MaxAndSkipEnv(env, skip=frameskip)\n",
        "        env = gym.wrappers.TransformObservation(\n",
        "            env,\n",
        "            lambda obs: obs[:, :, ::-1] if obs is not None and obs.ndim == 3 else obs,\n",
        "            observation_space=env.observation_space,\n",
        "        )\n",
        "        env = FrameProcessor(env)\n",
        "        env = FrameStack(env, k=frame_stack)\n",
        "    else:\n",
        "        env = gym.wrappers.NormalizeObservation(env)\n",
        "        env = gym.wrappers.TransformObservation(\n",
        "            env,\n",
        "            lambda x: x.astype(np.float32),\n",
        "            observation_space=gym.spaces.Box(\n",
        "                low=-np.inf,\n",
        "                high=np.inf,\n",
        "                shape=env.observation_space.shape,\n",
        "                dtype=np.float32,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(args):\n",
        "    env = make_env(args.env, args.seed, args.cnn, args.frameskip, args.frame_stack)\n",
        "    obs_shape = env.observation_space.shape\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    if args.cnn:\n",
        "        obs_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # HWC->CHW\n",
        "        q_net = DQN_CNN(obs_shape, act_dim)\n",
        "        target_q_net = DQN_CNN(obs_shape, act_dim)\n",
        "    else:\n",
        "        q_net = DQN_MLP(obs_shape[0], act_dim)\n",
        "        target_q_net = DQN_MLP(obs_shape[0], act_dim)\n",
        "\n",
        "    target_q_net.load_state_dict(q_net.state_dict())\n",
        "    optimizer = optim.Adam(q_net.parameters(), lr=args.lr)\n",
        "    buffer = ReplayBuffer(args.buffer_size)\n",
        "\n",
        "    epsilon = args.eps_start\n",
        "    epsilon_decay = (args.eps_start - args.eps_end) / args.eps_decay_steps\n",
        "    global_step = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    obs, _ = env.reset(seed=args.seed)\n",
        "    if args.cnn:\n",
        "        obs = np.transpose(obs, (2, 0, 1))\n",
        "\n",
        "    while global_step < args.total_steps:\n",
        "        # Epsilon-greedy\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "                if args.cnn:\n",
        "                    obs_t = obs_t.to(next(q_net.parameters()).device)\n",
        "                q_values = q_net(obs_t)\n",
        "                action = q_values.argmax(dim=1).item()\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if args.cnn:\n",
        "            next_obs_proc = np.transpose(next_obs, (2, 0, 1))\n",
        "            buffer.push(obs, action, reward, next_obs_proc, done)\n",
        "            obs = next_obs_proc\n",
        "        else:\n",
        "            buffer.push(obs, action, reward, next_obs, done)\n",
        "            obs = next_obs\n",
        "\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "            if args.cnn:\n",
        "                obs = np.transpose(obs, (2, 0, 1))\n",
        "            if \"episode\" in info:\n",
        "                ep_r = info[\"episode\"][\"r\"]\n",
        "                episode_rewards.append(ep_r)\n",
        "                if len(episode_rewards) % args.log_interval == 0:\n",
        "                    avg_r = np.mean(episode_rewards[-args.log_interval :])\n",
        "                    print(f\"Step {global_step}, AvgReward {avg_r:.2f}, Eps {epsilon:.3f}\")\n",
        "\n",
        "        # Training step\n",
        "        if (\n",
        "            len(buffer) >= args.learning_starts\n",
        "            and global_step % args.train_freq == 0\n",
        "        ):\n",
        "            (\n",
        "                batch_obs,\n",
        "                batch_actions,\n",
        "                batch_rewards,\n",
        "                batch_next_obs,\n",
        "                batch_dones,\n",
        "            ) = buffer.sample(args.batch_size)\n",
        "\n",
        "            batch_obs_t = torch.tensor(batch_obs, dtype=torch.float32)\n",
        "            batch_actions_t = torch.tensor(batch_actions, dtype=torch.int64)\n",
        "            batch_rewards_t = torch.tensor(batch_rewards, dtype=torch.float32)\n",
        "            batch_next_obs_t = torch.tensor(batch_next_obs, dtype=torch.float32)\n",
        "            batch_dones_t = torch.tensor(batch_dones, dtype=torch.float32)\n",
        "\n",
        "            if args.cnn:\n",
        "                batch_obs_t = batch_obs_t / 255.0\n",
        "                batch_next_obs_t = batch_next_obs_t / 255.0\n",
        "\n",
        "            q_values = q_net(batch_obs_t).gather(1, batch_actions_t.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if args.double:\n",
        "                    next_actions = q_net(batch_next_obs_t).argmax(dim=1)\n",
        "                    next_q = target_q_net(batch_next_obs_t).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "                else:\n",
        "                    next_q = target_q_net(batch_next_obs_t).max(1)[0]\n",
        "                target = batch_rewards_t + args.gamma * (1 - batch_dones_t) * next_q\n",
        "\n",
        "            loss = nn.SmoothL1Loss()(q_values, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(q_net.parameters(), 10.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        if global_step % args.target_update == 0:\n",
        "            target_q_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        epsilon = max(args.eps_end, epsilon - epsilon_decay)\n",
        "        global_step += 1\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--env\", type=str, default=\"CartPole-v1\")\n",
        "    parser.add_argument(\"--total-steps\", type=int, default=5000)  # shorter for Colab demo\n",
        "    parser.add_argument(\"--buffer-size\", type=int, default=10000)\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=64)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--eps-start\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--eps-end\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--eps-decay-steps\", type=int, default=1000)\n",
        "    parser.add_argument(\"--target-update\", type=int, default=1000)\n",
        "    parser.add_argument(\"--learning-starts\", type=int, default=1000)\n",
        "    parser.add_argument(\"--train-freq\", type=int, default=1)\n",
        "    parser.add_argument(\"--cnn\", action=\"store_true\")\n",
        "    parser.add_argument(\"--frameskip\", type=int, default=4)\n",
        "    parser.add_argument(\"--frame-stack\", type=int, default=4)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--double\", action=\"store_true\")\n",
        "    parser.add_argument(\"--log-interval\", type=int, default=10)\n",
        "\n",
        "    # âœ… Ignore Jupyter/Colab args\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    train(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pwLc5QS3fUa",
        "outputId": "d21593be-f102-403d-f9bb-95e603a75f52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200, AvgReward 20.10, Eps 0.820\n",
            "Step 367, AvgReward 16.70, Eps 0.670\n",
            "Step 525, AvgReward 15.80, Eps 0.527\n",
            "Step 660, AvgReward 13.50, Eps 0.406\n",
            "Step 789, AvgReward 12.90, Eps 0.290\n",
            "Step 891, AvgReward 10.20, Eps 0.198\n",
            "Step 1000, AvgReward 10.90, Eps 0.100\n",
            "Step 1108, AvgReward 10.80, Eps 0.100\n",
            "Step 1213, AvgReward 10.50, Eps 0.100\n",
            "Step 1310, AvgReward 9.70, Eps 0.100\n",
            "Step 1403, AvgReward 9.30, Eps 0.100\n",
            "Step 1502, AvgReward 9.90, Eps 0.100\n",
            "Step 1600, AvgReward 9.80, Eps 0.100\n",
            "Step 1698, AvgReward 9.80, Eps 0.100\n",
            "Step 1798, AvgReward 10.00, Eps 0.100\n",
            "Step 1898, AvgReward 10.00, Eps 0.100\n",
            "Step 1991, AvgReward 9.30, Eps 0.100\n",
            "Step 2110, AvgReward 11.90, Eps 0.100\n",
            "Step 2210, AvgReward 10.00, Eps 0.100\n",
            "Step 2328, AvgReward 11.80, Eps 0.100\n",
            "Step 2495, AvgReward 16.70, Eps 0.100\n",
            "Step 2654, AvgReward 15.90, Eps 0.100\n",
            "Step 2766, AvgReward 11.20, Eps 0.100\n",
            "Step 2903, AvgReward 13.70, Eps 0.100\n",
            "Step 3060, AvgReward 15.70, Eps 0.100\n",
            "Step 3270, AvgReward 21.00, Eps 0.100\n",
            "Step 3468, AvgReward 19.80, Eps 0.100\n",
            "Step 3697, AvgReward 22.90, Eps 0.100\n",
            "Step 3908, AvgReward 21.10, Eps 0.100\n",
            "Step 4194, AvgReward 28.60, Eps 0.100\n",
            "Step 4589, AvgReward 39.50, Eps 0.100\n",
            "Step 4823, AvgReward 23.40, Eps 0.100\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing a policy gradient algorithm\n",
        "from __future__ import annotations\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# -----------------------\n",
        "# Utilities\n",
        "# -----------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def discounted_returns(rewards: List[float], gamma: float, reward_to_go: bool) -> List[float]:\n",
        "    \"\"\"Compute (reward-to-go) discounted returns for a single episode.\"\"\"\n",
        "    if reward_to_go:\n",
        "        returns = np.zeros(len(rewards), dtype=np.float32)\n",
        "        running = 0.0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            running = rewards[t] + gamma * running\n",
        "            returns[t] = running\n",
        "        return returns.tolist()\n",
        "    else:\n",
        "        # full-episode return repeated\n",
        "        total = 0.0\n",
        "        for r in rewards:\n",
        "            total = total + r * (gamma ** 0)  # just sum (no discount structure needed here)\n",
        "        # but we want discounted sum for the episode: compute properly:\n",
        "        running = 0.0\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            running = rewards[t] + gamma * running\n",
        "        return [float(running)] * len(rewards)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Networks\n",
        "# -----------------------\n",
        "class MLPPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim: int, act_dim: int, hidden_sizes=(64, 64)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, act_dim))\n",
        "        self.logits_net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.logits_net(x)  # raw logits\n",
        "\n",
        "    def get_action_and_logp(self, obs: np.ndarray, device: torch.device):\n",
        "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
        "        if obs_t.dim() == 1:\n",
        "            obs_t = obs_t.unsqueeze(0)\n",
        "        logits = self.forward(obs_t)\n",
        "        dist = Categorical(logits=logits)\n",
        "        action = dist.sample()\n",
        "        logp = dist.log_prob(action)\n",
        "        return int(action.item()), float(logp.item())\n",
        "\n",
        "\n",
        "class MLPValue(nn.Module):\n",
        "    def __init__(self, obs_dim: int, hidden_sizes=(64, 64)):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        last = obs_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(last, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, 1))\n",
        "        self.v_net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.v_net(x).squeeze(-1)  # returns shape (batch,)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Training loops\n",
        "# -----------------------\n",
        "@dataclass\n",
        "class PGConfig:\n",
        "    env: str = \"CartPole-v1\"\n",
        "    algo: str = \"reinforce\"  # \"reinforce\" or \"actor_critic\"\n",
        "    total_timesteps: int = 200_000\n",
        "    batch_size: int = 5000  # for REINFORCE: number of timesteps per policy update\n",
        "    max_episode_len: int = 1000\n",
        "    gamma: float = 0.99\n",
        "    lr: float = 1e-3\n",
        "    hidden_sizes: Tuple[int, int] = (64, 64)\n",
        "    seed: int = 0\n",
        "    reward_to_go: bool = True\n",
        "    normalize_adv: bool = True\n",
        "    entropy_coef: float = 0.0\n",
        "    value_lr: float = 1e-3  # for actor-critic\n",
        "    value_iters: int = 1  # how many value updates per actor update\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    save_path: str = \"pg_checkpoint.pt\"\n",
        "    log_interval: int = 10\n",
        "\n",
        "\n",
        "def train_reinforce(cfg: PGConfig):\n",
        "    env = gym.make(cfg.env)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=cfg.hidden_sizes).to(cfg.device)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=cfg.lr)\n",
        "\n",
        "    total_steps = 0\n",
        "    ep_returns = []\n",
        "    ep_lens = []\n",
        "    logs = []\n",
        "\n",
        "    while total_steps < cfg.total_timesteps:\n",
        "        # collect a batch of trajectories (timesteps >= batch_size)\n",
        "        batch_obs = []\n",
        "        batch_acts = []\n",
        "        batch_logps = []\n",
        "        batch_rets = []  # discounted returns for each timestep\n",
        "        batch_lens = []\n",
        "\n",
        "        steps_collected = 0\n",
        "        while steps_collected < cfg.batch_size:\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            ep_rewards = []\n",
        "            ep_obs = []\n",
        "            ep_acts = []\n",
        "            ep_logps = []\n",
        "            for t in range(cfg.max_episode_len):\n",
        "                a, logp = policy.get_action_and_logp(obs, device=cfg.device)\n",
        "                next_obs, r, terminated, truncated, info = env.step(a)\n",
        "                done = terminated or truncated\n",
        "                ep_obs.append(obs.copy())\n",
        "                ep_acts.append(a)\n",
        "                ep_logps.append(logp)\n",
        "                ep_rewards.append(r)\n",
        "                obs = next_obs\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # compute discounted returns\n",
        "            rets = discounted_returns(ep_rewards, cfg.gamma, reward_to_go=cfg.reward_to_go)\n",
        "            batch_obs.extend(ep_obs)\n",
        "            batch_acts.extend(ep_acts)\n",
        "            batch_logps.extend(ep_logps)\n",
        "            batch_rets.extend(rets)\n",
        "            batch_lens.append(len(ep_rewards))\n",
        "            ep_returns.append(sum(ep_rewards))\n",
        "            ep_lens.append(len(ep_rewards))\n",
        "            steps_collected += len(ep_rewards)\n",
        "            total_steps += len(ep_rewards)\n",
        "\n",
        "        # convert to tensors\n",
        "        obs_t = torch.as_tensor(np.array(batch_obs, dtype=np.float32), device=cfg.device)\n",
        "        acts_t = torch.as_tensor(np.array(batch_acts, dtype=np.int64), device=cfg.device)\n",
        "        logps_old_t = torch.as_tensor(np.array(batch_logps, dtype=np.float32), device=cfg.device)\n",
        "        rets_t = torch.as_tensor(np.array(batch_rets, dtype=np.float32), device=cfg.device)\n",
        "\n",
        "        # optionally normalize returns (baseline of zero)\n",
        "        if cfg.normalize_adv:\n",
        "            rets_t = (rets_t - rets_t.mean()) / (rets_t.std() + 1e-8)\n",
        "\n",
        "        # policy loss = - sum log pi(a|s) * G_t  (averaged)\n",
        "        logits = policy(obs_t)\n",
        "        dist = Categorical(logits=logits)\n",
        "        logp_all = dist.log_prob(acts_t)\n",
        "        entropy = dist.entropy().mean()\n",
        "        loss = -(logp_all * rets_t).mean() - cfg.entropy_coef * entropy\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if len(ep_returns) >= cfg.log_interval:\n",
        "            avg_return = np.mean(ep_returns[-cfg.log_interval:])\n",
        "            avg_len = np.mean(ep_lens[-cfg.log_interval:])\n",
        "            print(f\"[REINFORCE] steps={total_steps} avg_return={avg_return:.2f} avg_len={avg_len:.2f} loss={loss.item():.4f}\")\n",
        "            logs.append((total_steps, avg_return, avg_len))\n",
        "    env.close()\n",
        "    # save\n",
        "    torch.save({\"policy_state\": policy.state_dict()}, cfg.save_path)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_actor_critic(cfg: PGConfig):\n",
        "    env = gym.make(cfg.env)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    policy = MLPPolicy(obs_dim, act_dim, hidden_sizes=cfg.hidden_sizes).to(cfg.device)\n",
        "    value = MLPValue(obs_dim, hidden_sizes=cfg.hidden_sizes).to(cfg.device)\n",
        "\n",
        "    opt_policy = optim.Adam(policy.parameters(), lr=cfg.lr)\n",
        "    opt_value = optim.Adam(value.parameters(), lr=cfg.value_lr)\n",
        "\n",
        "    total_steps = 0\n",
        "    ep_returns = []\n",
        "    ep_lens = []\n",
        "    logs = []\n",
        "\n",
        "    while total_steps < cfg.total_timesteps:\n",
        "        # collect batch of transitions (here we collect episodes until steps >= batch_size)\n",
        "        batch_obs = []\n",
        "        batch_acts = []\n",
        "        batch_logps = []\n",
        "        batch_rets = []\n",
        "        batch_vals = []\n",
        "        batch_lens = []\n",
        "\n",
        "        steps_collected = 0\n",
        "        while steps_collected < cfg.batch_size:\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            ep_rewards = []\n",
        "            ep_obs = []\n",
        "            ep_acts = []\n",
        "            ep_logps = []\n",
        "            ep_vals = []\n",
        "            for t in range(cfg.max_episode_len):\n",
        "                obs_t = torch.as_tensor(obs[None, :], dtype=torch.float32, device=cfg.device)\n",
        "                logits = policy(obs_t)\n",
        "                dist = Categorical(logits=logits)\n",
        "                action = dist.sample().item()\n",
        "                logp = float(dist.log_prob(torch.tensor(action, device=cfg.device)).item())\n",
        "                val = float(value(obs_t).item())\n",
        "\n",
        "                next_obs, r, terminated, truncated, info = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                ep_obs.append(obs.copy())\n",
        "                ep_acts.append(action)\n",
        "                ep_logps.append(logp)\n",
        "                ep_vals.append(val)\n",
        "                ep_rewards.append(r)\n",
        "\n",
        "                obs = next_obs\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            returns = discounted_returns(ep_rewards, cfg.gamma, reward_to_go=cfg.reward_to_go)\n",
        "            batch_obs.extend(ep_obs)\n",
        "            batch_acts.extend(ep_acts)\n",
        "            batch_logps.extend(ep_logps)\n",
        "            batch_rets.extend(returns)\n",
        "            batch_vals.extend(ep_vals)\n",
        "            batch_lens.append(len(ep_rewards))\n",
        "            ep_returns.append(sum(ep_rewards))\n",
        "            ep_lens.append(len(ep_rewards))\n",
        "            steps_collected += len(ep_rewards)\n",
        "            total_steps += len(ep_rewards)\n",
        "\n",
        "        # tensors\n",
        "        obs_t = torch.as_tensor(np.array(batch_obs, dtype=np.float32), device=cfg.device)\n",
        "        acts_t = torch.as_tensor(np.array(batch_acts, dtype=np.int64), device=cfg.device)\n",
        "        rets_t = torch.as_tensor(np.array(batch_rets, dtype=np.float32), device=cfg.device)\n",
        "        vals_t = torch.as_tensor(np.array(batch_vals, dtype=np.float32), device=cfg.device)\n",
        "\n",
        "        # advantages\n",
        "        adv_t = rets_t - vals_t\n",
        "        if cfg.normalize_adv:\n",
        "            adv_t = (adv_t - adv_t.mean()) / (adv_t.std() + 1e-8)\n",
        "\n",
        "        # policy loss (with entropy bonus)\n",
        "        logits = policy(obs_t)\n",
        "        dist = Categorical(logits=logits)\n",
        "        logp_all = dist.log_prob(acts_t)\n",
        "        entropy = dist.entropy().mean()\n",
        "        policy_loss = -(logp_all * adv_t).mean() - cfg.entropy_coef * entropy\n",
        "\n",
        "        opt_policy.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        opt_policy.step()\n",
        "\n",
        "        # value loss (MSE between returns and value)\n",
        "        for _ in range(cfg.value_iters):\n",
        "            val_pred = value(obs_t)\n",
        "            value_loss = nn.MSELoss()(val_pred, rets_t)\n",
        "            opt_value.zero_grad()\n",
        "            value_loss.backward()\n",
        "            opt_value.step()\n",
        "\n",
        "        if len(ep_returns) >= cfg.log_interval:\n",
        "            avg_return = np.mean(ep_returns[-cfg.log_interval:])\n",
        "            avg_len = np.mean(ep_lens[-cfg.log_interval:])\n",
        "            print(f\"[A2C] steps={total_steps} avg_return={avg_return:.2f} avg_len={avg_len:.2f} policy_loss={policy_loss.item():.4f} value_loss={value_loss.item():.4f}\")\n",
        "            logs.append((total_steps, avg_return, avg_len))\n",
        "\n",
        "    env.close()\n",
        "    torch.save({\"policy_state\": policy.state_dict(), \"value_state\": value.state_dict()}, cfg.save_path)\n",
        "    return logs\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# CLI / Entry\n",
        "# -----------------------\n",
        "def parse_args_colab_safe():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--env\", type=str, default=\"CartPole-v1\")\n",
        "    p.add_argument(\"--algo\", type=str, default=\"reinforce\", choices=[\"reinforce\", \"actor_critic\"])\n",
        "    p.add_argument(\"--total-timesteps\", type=int, default=200000)\n",
        "    p.add_argument(\"--batch-size\", type=int, default=5000)\n",
        "    p.add_argument(\"--max-episode-len\", type=int, default=1000)\n",
        "    p.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--value-lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--hidden-sizes\", nargs=\"+\", type=int, default=[64, 64])\n",
        "    p.add_argument(\"--seed\", type=int, default=0)\n",
        "    p.add_argument(\"--reward-to-go\", action=\"store_true\")\n",
        "    p.add_argument(\"--no-normalize-adv\", dest=\"normalize_adv\", action=\"store_false\")\n",
        "    p.add_argument(\"--entropy-coef\", type=float, default=0.0)\n",
        "    p.add_argument(\"--value-iters\", type=int, default=1)\n",
        "    p.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    p.add_argument(\"--save-path\", type=str, default=\"pg_checkpoint.pt\")\n",
        "    p.add_argument(\"--log-interval\", type=int, default=10)\n",
        "\n",
        "    # ignore Jupyter / Colab args if present\n",
        "    args = p.parse_args(args=[])\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args_colab_safe()\n",
        "    cfg = PGConfig(\n",
        "        env=args.env,\n",
        "        algo=args.algo,\n",
        "        total_timesteps=args.total_timesteps,\n",
        "        batch_size=args.batch_size,\n",
        "        max_episode_len=args.max_episode_len,\n",
        "        gamma=args.gamma,\n",
        "        lr=args.lr,\n",
        "        hidden_sizes=tuple(args.hidden_sizes),\n",
        "        seed=args.seed,\n",
        "        reward_to_go=args.reward_to_go,\n",
        "        normalize_adv=args.normalize_adv,\n",
        "        entropy_coef=args.entropy_coef,\n",
        "        value_lr=args.value_lr,\n",
        "        value_iters=args.value_iters,\n",
        "        device=args.device,\n",
        "        save_path=args.save_path,\n",
        "        log_interval=args.log_interval,\n",
        "    )\n",
        "\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    print(f\"Running {cfg.algo} on {cfg.env} for {cfg.total_timesteps} timesteps (device={cfg.device})\")\n",
        "\n",
        "    if cfg.algo == \"reinforce\":\n",
        "        train_reinforce(cfg)\n",
        "    else:\n",
        "        train_actor_critic(cfg)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGkGxLJu7Q5L",
        "outputId": "01800067-2e0f-49a7-90a4-3552200fc560"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running reinforce on CartPole-v1 for 200000 timesteps (device=cpu)\n",
            "[REINFORCE] steps=5027 avg_return=26.70 avg_len=26.70 loss=0.0023\n",
            "[REINFORCE] steps=10028 avg_return=19.40 avg_len=19.40 loss=0.0002\n",
            "[REINFORCE] steps=15038 avg_return=24.30 avg_len=24.30 loss=0.0008\n",
            "[REINFORCE] steps=20046 avg_return=22.20 avg_len=22.20 loss=-0.0017\n",
            "[REINFORCE] steps=25087 avg_return=22.60 avg_len=22.60 loss=-0.0062\n",
            "[REINFORCE] steps=30110 avg_return=26.50 avg_len=26.50 loss=-0.0056\n",
            "[REINFORCE] steps=35124 avg_return=32.00 avg_len=32.00 loss=-0.0049\n",
            "[REINFORCE] steps=40163 avg_return=24.20 avg_len=24.20 loss=-0.0050\n",
            "[REINFORCE] steps=45194 avg_return=24.70 avg_len=24.70 loss=-0.0072\n",
            "[REINFORCE] steps=50195 avg_return=28.00 avg_len=28.00 loss=-0.0095\n",
            "[REINFORCE] steps=55203 avg_return=24.10 avg_len=24.10 loss=-0.0088\n",
            "[REINFORCE] steps=60235 avg_return=36.90 avg_len=36.90 loss=-0.0083\n",
            "[REINFORCE] steps=65247 avg_return=29.40 avg_len=29.40 loss=-0.0131\n",
            "[REINFORCE] steps=70272 avg_return=23.20 avg_len=23.20 loss=-0.0106\n",
            "[REINFORCE] steps=75290 avg_return=30.50 avg_len=30.50 loss=-0.0115\n",
            "[REINFORCE] steps=80304 avg_return=34.70 avg_len=34.70 loss=-0.0113\n",
            "[REINFORCE] steps=85329 avg_return=39.20 avg_len=39.20 loss=-0.0132\n",
            "[REINFORCE] steps=90386 avg_return=34.10 avg_len=34.10 loss=-0.0148\n",
            "[REINFORCE] steps=95399 avg_return=29.20 avg_len=29.20 loss=-0.0115\n",
            "[REINFORCE] steps=100410 avg_return=37.00 avg_len=37.00 loss=-0.0138\n",
            "[REINFORCE] steps=105426 avg_return=26.60 avg_len=26.60 loss=-0.0142\n",
            "[REINFORCE] steps=110470 avg_return=49.20 avg_len=49.20 loss=-0.0207\n",
            "[REINFORCE] steps=115484 avg_return=28.20 avg_len=28.20 loss=-0.0099\n",
            "[REINFORCE] steps=120498 avg_return=32.70 avg_len=32.70 loss=-0.0124\n",
            "[REINFORCE] steps=125551 avg_return=48.10 avg_len=48.10 loss=-0.0146\n",
            "[REINFORCE] steps=130566 avg_return=33.20 avg_len=33.20 loss=-0.0163\n",
            "[REINFORCE] steps=135583 avg_return=48.80 avg_len=48.80 loss=-0.0175\n",
            "[REINFORCE] steps=140604 avg_return=32.50 avg_len=32.50 loss=-0.0119\n",
            "[REINFORCE] steps=145610 avg_return=31.40 avg_len=31.40 loss=-0.0197\n",
            "[REINFORCE] steps=150630 avg_return=47.40 avg_len=47.40 loss=-0.0171\n",
            "[REINFORCE] steps=155633 avg_return=50.10 avg_len=50.10 loss=-0.0121\n",
            "[REINFORCE] steps=160674 avg_return=46.90 avg_len=46.90 loss=-0.0178\n",
            "[REINFORCE] steps=165709 avg_return=50.50 avg_len=50.50 loss=-0.0144\n",
            "[REINFORCE] steps=170709 avg_return=50.90 avg_len=50.90 loss=-0.0114\n",
            "[REINFORCE] steps=175709 avg_return=55.20 avg_len=55.20 loss=-0.0082\n",
            "[REINFORCE] steps=180730 avg_return=45.40 avg_len=45.40 loss=-0.0165\n",
            "[REINFORCE] steps=185775 avg_return=57.90 avg_len=57.90 loss=-0.0139\n",
            "[REINFORCE] steps=190803 avg_return=49.60 avg_len=49.60 loss=-0.0121\n",
            "[REINFORCE] steps=195803 avg_return=43.10 avg_len=43.10 loss=-0.0198\n",
            "[REINFORCE] steps=200854 avg_return=53.30 avg_len=53.30 loss=-0.0124\n"
          ]
        }
      ]
    }
  ]
}