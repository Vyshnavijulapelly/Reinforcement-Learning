{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVrzbJNtQsIei0AvkbGelB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavijulapelly/Reinforcement-Learning/blob/main/RL_Lab_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return (\n",
        "            np.stack(state),\n",
        "            np.array(action),\n",
        "            np.array(reward, dtype=np.float32),\n",
        "            np.stack(next_state),\n",
        "            np.array(done, dtype=np.uint8),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# Q-networks\n",
        "class DQN_MLP(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, act_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class DQN_CNN(nn.Module):\n",
        "    def __init__(self, input_shape, act_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_shape\n",
        "        assert h == 84 and w == 84, \"CNN expects 84x84 input\"\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, act_dim),\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / 255.0\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "\n",
        "# Atari preprocessing wrappers\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "        self._obs_buffer = np.zeros(\n",
        "            (2,) + env.observation_space.shape, dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            if i == self._skip - 2:\n",
        "                self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1:\n",
        "                self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "        return max_frame, total_reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class FrameProcessor(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "        return np.expand_dims(obs, -1)\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        super().__init__(env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(obs)\n",
        "        return self._get_obs(), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.frames.append(obs)\n",
        "        return self._get_obs(), reward, terminated, truncated, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.concatenate(list(self.frames), axis=-1)\n",
        "\n",
        "\n",
        "# Environment builder\n",
        "def make_env(env_id: str, seed: int, cnn: bool, frameskip: int, frame_stack: int):\n",
        "    env = gym.make(env_id, frameskip=1) if \"ALE/\" in env_id else gym.make(env_id)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    if cnn:\n",
        "        if \"ALE/\" not in env_id:\n",
        "            raise ValueError(\"--cnn was set but env is not Atari. Use ALE/Breakout-v5 etc.\")\n",
        "        env = MaxAndSkipEnv(env, skip=frameskip)\n",
        "        env = gym.wrappers.TransformObservation(\n",
        "            env,\n",
        "            lambda obs: obs[:, :, ::-1] if obs is not None and obs.ndim == 3 else obs,\n",
        "            observation_space=env.observation_space,\n",
        "        )\n",
        "        env = FrameProcessor(env)\n",
        "        env = FrameStack(env, k=frame_stack)\n",
        "    else:\n",
        "        env = gym.wrappers.NormalizeObservation(env)\n",
        "        env = gym.wrappers.TransformObservation(\n",
        "            env,\n",
        "            lambda x: x.astype(np.float32),\n",
        "            observation_space=gym.spaces.Box(\n",
        "                low=-np.inf,\n",
        "                high=np.inf,\n",
        "                shape=env.observation_space.shape,\n",
        "                dtype=np.float32,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "    return env\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(args):\n",
        "    env = make_env(args.env, args.seed, args.cnn, args.frameskip, args.frame_stack)\n",
        "    obs_shape = env.observation_space.shape\n",
        "    act_dim = env.action_space.n\n",
        "\n",
        "    if args.cnn:\n",
        "        obs_shape = (obs_shape[2], obs_shape[0], obs_shape[1])  # HWC->CHW\n",
        "        q_net = DQN_CNN(obs_shape, act_dim)\n",
        "        target_q_net = DQN_CNN(obs_shape, act_dim)\n",
        "    else:\n",
        "        q_net = DQN_MLP(obs_shape[0], act_dim)\n",
        "        target_q_net = DQN_MLP(obs_shape[0], act_dim)\n",
        "\n",
        "    target_q_net.load_state_dict(q_net.state_dict())\n",
        "    optimizer = optim.Adam(q_net.parameters(), lr=args.lr)\n",
        "    buffer = ReplayBuffer(args.buffer_size)\n",
        "\n",
        "    epsilon = args.eps_start\n",
        "    epsilon_decay = (args.eps_start - args.eps_end) / args.eps_decay_steps\n",
        "    global_step = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    obs, _ = env.reset(seed=args.seed)\n",
        "    if args.cnn:\n",
        "        obs = np.transpose(obs, (2, 0, 1))\n",
        "\n",
        "    while global_step < args.total_steps:\n",
        "        # Epsilon-greedy\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
        "                if args.cnn:\n",
        "                    obs_t = obs_t.to(next(q_net.parameters()).device)\n",
        "                q_values = q_net(obs_t)\n",
        "                action = q_values.argmax(dim=1).item()\n",
        "\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if args.cnn:\n",
        "            next_obs_proc = np.transpose(next_obs, (2, 0, 1))\n",
        "            buffer.push(obs, action, reward, next_obs_proc, done)\n",
        "            obs = next_obs_proc\n",
        "        else:\n",
        "            buffer.push(obs, action, reward, next_obs, done)\n",
        "            obs = next_obs\n",
        "\n",
        "        if done:\n",
        "            obs, _ = env.reset()\n",
        "            if args.cnn:\n",
        "                obs = np.transpose(obs, (2, 0, 1))\n",
        "            if \"episode\" in info:\n",
        "                ep_r = info[\"episode\"][\"r\"]\n",
        "                episode_rewards.append(ep_r)\n",
        "                if len(episode_rewards) % args.log_interval == 0:\n",
        "                    avg_r = np.mean(episode_rewards[-args.log_interval :])\n",
        "                    print(f\"Step {global_step}, AvgReward {avg_r:.2f}, Eps {epsilon:.3f}\")\n",
        "\n",
        "        # Training step\n",
        "        if (\n",
        "            len(buffer) >= args.learning_starts\n",
        "            and global_step % args.train_freq == 0\n",
        "        ):\n",
        "            (\n",
        "                batch_obs,\n",
        "                batch_actions,\n",
        "                batch_rewards,\n",
        "                batch_next_obs,\n",
        "                batch_dones,\n",
        "            ) = buffer.sample(args.batch_size)\n",
        "\n",
        "            batch_obs_t = torch.tensor(batch_obs, dtype=torch.float32)\n",
        "            batch_actions_t = torch.tensor(batch_actions, dtype=torch.int64)\n",
        "            batch_rewards_t = torch.tensor(batch_rewards, dtype=torch.float32)\n",
        "            batch_next_obs_t = torch.tensor(batch_next_obs, dtype=torch.float32)\n",
        "            batch_dones_t = torch.tensor(batch_dones, dtype=torch.float32)\n",
        "\n",
        "            if args.cnn:\n",
        "                batch_obs_t = batch_obs_t / 255.0\n",
        "                batch_next_obs_t = batch_next_obs_t / 255.0\n",
        "\n",
        "            q_values = q_net(batch_obs_t).gather(1, batch_actions_t.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if args.double:\n",
        "                    next_actions = q_net(batch_next_obs_t).argmax(dim=1)\n",
        "                    next_q = target_q_net(batch_next_obs_t).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "                else:\n",
        "                    next_q = target_q_net(batch_next_obs_t).max(1)[0]\n",
        "                target = batch_rewards_t + args.gamma * (1 - batch_dones_t) * next_q\n",
        "\n",
        "            loss = nn.SmoothL1Loss()(q_values, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(q_net.parameters(), 10.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        if global_step % args.target_update == 0:\n",
        "            target_q_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "        epsilon = max(args.eps_end, epsilon - epsilon_decay)\n",
        "        global_step += 1\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--env\", type=str, default=\"CartPole-v1\")\n",
        "    parser.add_argument(\"--total-steps\", type=int, default=5000)  # shorter for Colab demo\n",
        "    parser.add_argument(\"--buffer-size\", type=int, default=10000)\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=64)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
        "    parser.add_argument(\"--eps-start\", type=float, default=1.0)\n",
        "    parser.add_argument(\"--eps-end\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--eps-decay-steps\", type=int, default=1000)\n",
        "    parser.add_argument(\"--target-update\", type=int, default=1000)\n",
        "    parser.add_argument(\"--learning-starts\", type=int, default=1000)\n",
        "    parser.add_argument(\"--train-freq\", type=int, default=1)\n",
        "    parser.add_argument(\"--cnn\", action=\"store_true\")\n",
        "    parser.add_argument(\"--frameskip\", type=int, default=4)\n",
        "    parser.add_argument(\"--frame-stack\", type=int, default=4)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--double\", action=\"store_true\")\n",
        "    parser.add_argument(\"--log-interval\", type=int, default=10)\n",
        "\n",
        "    # âœ… Ignore Jupyter/Colab args\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    train(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pwLc5QS3fUa",
        "outputId": "d21593be-f102-403d-f9bb-95e603a75f52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 200, AvgReward 20.10, Eps 0.820\n",
            "Step 367, AvgReward 16.70, Eps 0.670\n",
            "Step 525, AvgReward 15.80, Eps 0.527\n",
            "Step 660, AvgReward 13.50, Eps 0.406\n",
            "Step 789, AvgReward 12.90, Eps 0.290\n",
            "Step 891, AvgReward 10.20, Eps 0.198\n",
            "Step 1000, AvgReward 10.90, Eps 0.100\n",
            "Step 1108, AvgReward 10.80, Eps 0.100\n",
            "Step 1213, AvgReward 10.50, Eps 0.100\n",
            "Step 1310, AvgReward 9.70, Eps 0.100\n",
            "Step 1403, AvgReward 9.30, Eps 0.100\n",
            "Step 1502, AvgReward 9.90, Eps 0.100\n",
            "Step 1600, AvgReward 9.80, Eps 0.100\n",
            "Step 1698, AvgReward 9.80, Eps 0.100\n",
            "Step 1798, AvgReward 10.00, Eps 0.100\n",
            "Step 1898, AvgReward 10.00, Eps 0.100\n",
            "Step 1991, AvgReward 9.30, Eps 0.100\n",
            "Step 2110, AvgReward 11.90, Eps 0.100\n",
            "Step 2210, AvgReward 10.00, Eps 0.100\n",
            "Step 2328, AvgReward 11.80, Eps 0.100\n",
            "Step 2495, AvgReward 16.70, Eps 0.100\n",
            "Step 2654, AvgReward 15.90, Eps 0.100\n",
            "Step 2766, AvgReward 11.20, Eps 0.100\n",
            "Step 2903, AvgReward 13.70, Eps 0.100\n",
            "Step 3060, AvgReward 15.70, Eps 0.100\n",
            "Step 3270, AvgReward 21.00, Eps 0.100\n",
            "Step 3468, AvgReward 19.80, Eps 0.100\n",
            "Step 3697, AvgReward 22.90, Eps 0.100\n",
            "Step 3908, AvgReward 21.10, Eps 0.100\n",
            "Step 4194, AvgReward 28.60, Eps 0.100\n",
            "Step 4589, AvgReward 39.50, Eps 0.100\n",
            "Step 4823, AvgReward 23.40, Eps 0.100\n",
            "Training finished.\n"
          ]
        }
      ]
    }
  ]
}