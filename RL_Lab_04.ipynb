{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF6N4CMxKs64NHnnRjJ4YH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavijulapelly/Reinforcement-Learning/blob/main/RL_Lab_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFH8LFln31dq",
        "outputId": "d8e7a77e-d872-43af-a269-4d3f4fc027da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using backend: gymnasium\n",
            "Training FrozenLake-v1 | episodes=20000, alpha=0.8, gamma=0.95\n",
            "[  2000/20000] avg_return(last 200) = 0.075, epsilon=0.368\n",
            "[  4000/20000] avg_return(last 200) = 0.075, epsilon=0.135\n",
            "[  6000/20000] avg_return(last 200) = 0.165, epsilon=0.050\n",
            "[  8000/20000] avg_return(last 200) = 0.330, epsilon=0.018\n",
            "[ 10000/20000] avg_return(last 200) = 0.445, epsilon=0.010\n",
            "[ 12000/20000] avg_return(last 200) = 0.515, epsilon=0.010\n",
            "[ 14000/20000] avg_return(last 200) = 0.525, epsilon=0.010\n",
            "[ 16000/20000] avg_return(last 200) = 0.510, epsilon=0.010\n",
            "[ 18000/20000] avg_return(last 200) = 0.490, epsilon=0.010\n",
            "[ 20000/20000] avg_return(last 200) = 0.440, epsilon=0.010\n",
            "\n",
            "Evaluation (greedy policy):\n",
            "Average reward over 100 episodes: 0.660\n",
            "Average steps to termination: 45.8\n",
            "\n",
            "Tips for FrozenLake:\n",
            "- For is_slippery=1 (stochastic), increase episodes (>=20k) and use slow epsilon decay (e.g., 0.9997).\n",
            "- For is_slippery=0 (deterministic), 3k–10k episodes often suffice.\n",
            "- Consider gamma around 0.95–0.99 and alpha 0.5–0.9.\n",
            "\n",
            "Saved: q_table.npy, returns.npy, epsilons.npy\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import argparse  # Import argparse\n",
        "\n",
        "# Try Gymnasium first; fallback to Gym\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "    GYMN = \"gymnasium\"\n",
        "except Exception:\n",
        "    import gym\n",
        "    GYMN = \"gym\"\n",
        "\n",
        "\n",
        "def make_env(env_id: str,\n",
        "             is_slippery: bool | None = None,\n",
        "             render_mode: str | None = None,\n",
        "             seed: int | None = 42):\n",
        "    \"\"\"Create an environment with sensible defaults for FrozenLake and Taxi.\"\"\"\n",
        "    kwargs = {\"render_mode\": render_mode} if render_mode else {}\n",
        "    if env_id.startswith(\"FrozenLake\"):\n",
        "        if is_slippery is not None:\n",
        "            kwargs[\"is_slippery\"] = bool(is_slippery)\n",
        "    env = gym.make(env_id, **kwargs)\n",
        "    # Set seed for reproducibility where supported\n",
        "    try:\n",
        "        env.reset(seed=seed)\n",
        "        if hasattr(env, \"action_space\") and hasattr(env.action_space, \"seed\"):\n",
        "            env.action_space.seed(seed)\n",
        "        if hasattr(env, \"observation_space\") and hasattr(env.observation_space, \"seed\"):\n",
        "            env.observation_space.seed(seed)\n",
        "    except TypeError:\n",
        "        pass\n",
        "    return env\n",
        "\n",
        "\n",
        "def epsilon_greedy_action(q_table: np.ndarray, state: int, epsilon: float, n_actions: int) -> int:\n",
        "    if random.random() < epsilon:\n",
        "        return random.randrange(n_actions)\n",
        "    return int(np.argmax(q_table[state]))\n",
        "\n",
        "\n",
        "def train_q_learning(env_id: str = \"FrozenLake-v1\",\n",
        "                     episodes: int = 20000,\n",
        "                     max_steps: int = 200,\n",
        "                     alpha: float = 0.8,\n",
        "                     gamma: float = 0.95,\n",
        "                     epsilon: float = 1.0,\n",
        "                     epsilon_min: float = 0.01,\n",
        "                     epsilon_decay: float = 0.9995,\n",
        "                     is_slippery: bool | None = None,\n",
        "                     seed: int = 42,\n",
        "                     verbose: bool = True):\n",
        "    env = make_env(env_id, is_slippery=is_slippery, seed=seed)\n",
        "\n",
        "    assert hasattr(env.observation_space, 'n') and hasattr(env.action_space, 'n'), \\\n",
        "        \"This Q-learning implementation expects discrete state and action spaces.\"\n",
        "\n",
        "    n_states = env.observation_space.n\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    q_table = np.zeros((n_states, n_actions), dtype=np.float32)\n",
        "\n",
        "    returns = np.zeros(episodes, dtype=np.float32)\n",
        "    epsilons = np.zeros(episodes, dtype=np.float32)\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        # newer Gymnasium returns (obs, info)\n",
        "        reset_out = env.reset(seed=seed + ep)\n",
        "        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
        "\n",
        "        total_reward = 0.0\n",
        "        for t in range(max_steps):\n",
        "            action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n",
        "            step_out = env.step(action)\n",
        "            # Gymnasium: obs, reward, terminated, truncated, info\n",
        "            if len(step_out) == 5:\n",
        "                next_state, reward, terminated, truncated, _ = step_out\n",
        "                done = terminated or truncated\n",
        "            else:  # old Gym: obs, reward, done, info\n",
        "                next_state, reward, done, _ = step_out\n",
        "\n",
        "            # Q-learning update\n",
        "            best_next = np.max(q_table[next_state])\n",
        "            td_target = reward + gamma * best_next * (0 if done else 1)\n",
        "            td_error = td_target - q_table[state, action]\n",
        "            q_table[state, action] += alpha * td_error\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Book-keeping\n",
        "        returns[ep] = total_reward\n",
        "        epsilons[ep] = epsilon\n",
        "\n",
        "        # Decay epsilon\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "        # Progress log\n",
        "        if verbose and (ep + 1) % max(1, episodes // 10) == 0:\n",
        "            window = 200 if episodes >= 200 else max(1, episodes // 5)\n",
        "            recent_avg = float(np.mean(returns[max(0, ep - window + 1):ep + 1]))\n",
        "            print(f\"[{ep+1:6d}/{episodes}] avg_return(last {window}) = {recent_avg:.3f}, epsilon={epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return q_table, returns, epsilons\n",
        "\n",
        "\n",
        "def evaluate_policy(env_id: str,\n",
        "                    q_table: np.ndarray,\n",
        "                    episodes: int = 100,\n",
        "                    max_steps: int = 200,\n",
        "                    is_slippery: bool | None = None,\n",
        "                    seed: int = 9999):\n",
        "    env = make_env(env_id, is_slippery=is_slippery, render_mode=None, seed=seed)\n",
        "\n",
        "    total_rewards = []\n",
        "    steps_taken = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        reset_out = env.reset(seed=seed + ep)\n",
        "        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n",
        "        ep_reward = 0.0\n",
        "        for t in range(max_steps):\n",
        "            action = int(np.argmax(q_table[state]))\n",
        "            step_out = env.step(action)\n",
        "            if len(step_out) == 5:\n",
        "                next_state, reward, terminated, truncated, _ = step_out\n",
        "                done = terminated or truncated\n",
        "            else:\n",
        "                next_state, reward, done, _ = step_out\n",
        "\n",
        "            ep_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                steps_taken.append(t + 1)\n",
        "                break\n",
        "        else:\n",
        "            steps_taken.append(max_steps)\n",
        "        total_rewards.append(ep_reward)\n",
        "\n",
        "    env.close()\n",
        "    return float(np.mean(total_rewards)), float(np.mean(steps_taken))\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Tabular Q-Learning for discrete-action Gym environments\")\n",
        "    parser.add_argument('--env', type=str, default='FrozenLake-v1', help='Env id, e.g., FrozenLake-v1 or Taxi-v3')\n",
        "    parser.add_argument('--episodes', type=int, default=20000)\n",
        "    parser.add_argument('--max_steps', type=int, default=200)\n",
        "    parser.add_argument('--alpha', type=float, default=0.8, help='Learning rate')\n",
        "    parser.add_argument('--gamma', type=float, default=0.95, help='Discount factor')\n",
        "    parser.add_argument('--epsilon', type=float, default=1.0, help='Initial exploration rate')\n",
        "    parser.add_argument('--epsilon_min', type=float, default=0.01)\n",
        "    parser.add_argument('--epsilon_decay', type=float, default=0.9995)\n",
        "    parser.add_argument('--is_slippery', type=int, default=None, choices=[0,1], help='Only for FrozenLake: 1 or 0')\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--no_verbose', action='store_true', help='Disable training logs')\n",
        "\n",
        "    # ✅ FIX for Jupyter/Colab: ignore unknown args\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    print(f\"Using backend: {GYMN}\")\n",
        "    print(f\"Training {args.env} | episodes={args.episodes}, alpha={args.alpha}, gamma={args.gamma}\")\n",
        "\n",
        "    q_table, returns, eps = train_q_learning(env_id=args.env,\n",
        "                                             episodes=args.episodes,\n",
        "                                             max_steps=args.max_steps,\n",
        "                                             alpha=args.alpha,\n",
        "                                             gamma=args.gamma,\n",
        "                                             epsilon=args.epsilon,\n",
        "                                             epsilon_min=args.epsilon_min,\n",
        "                                             epsilon_decay=args.epsilon_decay,\n",
        "                                             is_slippery=(None if args.is_slippery is None else bool(args.is_slippery)),\n",
        "                                             seed=args.seed,\n",
        "                                             verbose=not args.no_verbose)\n",
        "\n",
        "    # Evaluation\n",
        "    avg_reward, avg_steps = evaluate_policy(env_id=args.env,\n",
        "                                            q_table=q_table,\n",
        "                                            max_steps=args.max_steps,\n",
        "                                            is_slippery=(None if args.is_slippery is None else bool(args.is_slippery)))\n",
        "\n",
        "    print(\"\\nEvaluation (greedy policy):\")\n",
        "    print(f\"Average reward over 100 episodes: {avg_reward:.3f}\")\n",
        "    print(f\"Average steps to termination: {avg_steps:.1f}\")\n",
        "\n",
        "    # Quick tips for typical setups\n",
        "    if args.env.startswith(\"FrozenLake\"):\n",
        "        print(\"\\nTips for FrozenLake:\")\n",
        "        print(\"- For is_slippery=1 (stochastic), increase episodes (>=20k) and use slow epsilon decay (e.g., 0.9997).\")\n",
        "        print(\"- For is_slippery=0 (deterministic), 3k–10k episodes often suffice.\")\n",
        "        print(\"- Consider gamma around 0.95–0.99 and alpha 0.5–0.9.\")\n",
        "    elif args.env.startswith(\"Taxi\"):\n",
        "        print(\"\\nTips for Taxi:\")\n",
        "        print(\"- Learns faster: 5k–10k episodes often work well.\")\n",
        "        print(\"- Smaller epsilon decay (e.g., 0.995) can speed up convergence.\")\n",
        "\n",
        "    # Save artifacts for later use\n",
        "    np.save(\"q_table.npy\", q_table)\n",
        "    np.save(\"returns.npy\", returns)\n",
        "    np.save(\"epsilons.npy\", eps)\n",
        "    print(\"\\nSaved: q_table.npy, returns.npy, epsilons.npy\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Interrupted by user.\")\n"
      ]
    }
  ]
}