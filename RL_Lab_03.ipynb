{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeC47BFJ+5Al6eSXiz3Dhd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavijulapelly/Reinforcement-Learning/blob/main/RL_Lab_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZOY73I3wmO6",
        "outputId": "6ff950ce-7dfe-4cc3-d337-c1e7f9eb482b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD(0) Value function estimate (V):\n",
            "[[0.01154136 0.00756639 0.02747178 0.013248  ]\n",
            " [0.01991066 0.         0.0626995  0.        ]\n",
            " [0.03684871 0.0758343  0.18880016 0.        ]\n",
            " [0.         0.16529731 0.52072376 0.        ]]\n",
            "\n",
            "Greedy policy from SARSA (actions 0:Left, 1:Down, 2:Right, 3:Up):\n",
            "[[0 3 1 3]\n",
            " [0 0 2 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            "\n",
            "State-action values Q[s,a] (reshaped per action for readability):\n",
            "Action 0:\n",
            "[[0.35879099 0.256256   0.2354679  0.12917957]\n",
            " [0.40523588 0.         0.15157585 0.        ]\n",
            " [0.2666601  0.318245   0.56247467 0.        ]\n",
            " [0.         0.30634394 0.63039223 0.        ]]\n",
            "Action 1:\n",
            "[[0.31427124 0.19830804 0.24873389 0.11229348]\n",
            " [0.23471058 0.         0.169121   0.        ]\n",
            " [0.26211337 0.56930223 0.30913391 0.        ]\n",
            " [0.         0.38829745 0.85720356 0.        ]]\n",
            "Action 2:\n",
            "[[0.2981604  0.10287446 0.2361989  0.14233501]\n",
            " [0.24663785 0.         0.21868749 0.        ]\n",
            " [0.25617631 0.40590125 0.30485473 0.        ]\n",
            " [0.         0.70056179 0.62130542 0.        ]]\n",
            "Action 3:\n",
            "[[0.31968015 0.26972596 0.23349976 0.22772739]\n",
            " [0.29724172 0.         0.05580837 0.        ]\n",
            " [0.47997678 0.27368312 0.16935533 0.        ]\n",
            " [0.         0.49462199 0.63985899 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import Callable, Tuple\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "\n",
        "def epsilon_greedy(Q: np.ndarray, s: int, epsilon: float) -> int:\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(Q.shape[1])\n",
        "    return int(np.argmax(Q[s]))\n",
        "\n",
        "def run_episode(env, policy: Callable[[int], int], gamma: float = 0.99, max_steps: int = 1000) -> Tuple[float, int]:\n",
        "    s, _ = env.reset()\n",
        "    total_reward, steps = 0.0, 0\n",
        "    for _ in range(max_steps):\n",
        "        a = policy(s)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        total_reward += r * (gamma ** steps)\n",
        "        steps += 1\n",
        "        s = s_next\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return total_reward, steps\n",
        "\n",
        "# ---------------------------\n",
        "# TD(0) Policy Evaluation\n",
        "# ---------------------------\n",
        "\n",
        "def td0_policy_evaluation(env_id: str = \"FrozenLake-v1\",\n",
        "                          is_slippery: bool = True,\n",
        "                          gamma: float = 0.99,\n",
        "                          alpha: float = 0.1,\n",
        "                          epsilon_random_policy: float = 1.0,\n",
        "                          episodes: int = 20_000,\n",
        "                          seed: int = 0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Estimates V^pi for a fixed policy using TD(0).\n",
        "    By default, pi is a fully random policy (epsilon_random_policy = 1.0).\n",
        "    \"\"\"\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    V = np.zeros(nS, dtype=np.float64)\n",
        "\n",
        "    def policy(s: int) -> int:\n",
        "        # Equiprobable random (or epsilon-random around greedy on V, but typically random)\n",
        "        if rng.random() < epsilon_random_policy:\n",
        "            return rng.integers(nA)\n",
        "        # If not fully random, do a crude one-step lookahead using V (optional)\n",
        "        return rng.integers(nA)\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = policy(s)\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            td_target = r + (0.0 if (terminated or truncated) else gamma * V[s_next])\n",
        "            V[s] += alpha * (td_target - V[s])\n",
        "            s = s_next\n",
        "            done = terminated or truncated\n",
        "\n",
        "    env.close()\n",
        "    return V\n",
        "\n",
        "# ---------------------------\n",
        "# SARSA (on-policy TD control)\n",
        "# ---------------------------\n",
        "\n",
        "def sarsa_control(env_id: str = \"FrozenLake-v1\",\n",
        "                  is_slippery: bool = True,\n",
        "                  gamma: float = 0.99,\n",
        "                  alpha: float = 0.1,\n",
        "                  epsilon_start: float = 1.0,\n",
        "                  epsilon_end: float = 0.05,\n",
        "                  epsilon_decay_steps: int = 50_000,\n",
        "                  episodes: int = 100_000,\n",
        "                  max_steps_per_ep: int = 200,\n",
        "                  seed: int = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Learns an epsilon-greedy policy for Q using on-policy SARSA.\n",
        "    Returns learned Q and the derived greedy policy (as a 1D array of actions).\n",
        "    \"\"\"\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    env.reset(seed=seed)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
        "\n",
        "    def epsilon_by_step(t: int) -> float:\n",
        "        # Linear decay\n",
        "        frac = min(1.0, max(0.0, t / max(1, epsilon_decay_steps)))\n",
        "        return epsilon_start + (epsilon_end - epsilon_start) * frac\n",
        "\n",
        "    timestep = 0\n",
        "    for ep in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        eps = epsilon_by_step(timestep)\n",
        "        a = rng.integers(nA) if rng.random() < eps else int(np.argmax(Q[s]))\n",
        "        for _ in range(max_steps_per_ep):\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            eps_next = epsilon_by_step(timestep + 1)\n",
        "            a_next = rng.integers(nA) if rng.random() < eps_next else int(np.argmax(Q[s_next]))\n",
        "            td_target = r + (0.0 if (terminated or truncated) else gamma * Q[s_next, a_next])\n",
        "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "\n",
        "            timestep += 1\n",
        "            s, a = s_next, a_next\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "    policy_greedy = np.argmax(Q, axis=1)\n",
        "    env.close()\n",
        "    return Q, policy_greedy\n",
        "\n",
        "# ---------------------------\n",
        "# Quick demo (optional)\n",
        "# ---------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TD(0) prediction under a random policy\n",
        "    V = td0_policy_evaluation(\n",
        "        env_id=\"FrozenLake-v1\",\n",
        "        is_slippery=True,   # set False for deterministic grid\n",
        "        gamma=0.99,\n",
        "        alpha=0.1,\n",
        "        epsilon_random_policy=1.0,\n",
        "        episodes=25_000,\n",
        "        seed=42\n",
        "    )\n",
        "    print(\"TD(0) Value function estimate (V):\")\n",
        "    print(V.reshape(4, 4))  # for 4x4 FrozenLake\n",
        "\n",
        "    # SARSA control to learn a policy\n",
        "    Q, pi = sarsa_control(\n",
        "        env_id=\"FrozenLake-v1\",\n",
        "        is_slippery=True,\n",
        "        gamma=0.99,\n",
        "        alpha=0.1,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.05,\n",
        "        epsilon_decay_steps=80_000,\n",
        "        episodes=120_000,\n",
        "        max_steps_per_ep=200,\n",
        "        seed=7\n",
        "    )\n",
        "    print(\"\\nGreedy policy from SARSA (actions 0:Left, 1:Down, 2:Right, 3:Up):\")\n",
        "    print(pi.reshape(4, 4))\n",
        "    print(\"\\nState-action values Q[s,a] (reshaped per action for readability):\")\n",
        "    for a in range(Q.shape[1]):\n",
        "        print(f\"Action {a}:\")\n",
        "        print(Q[:, a].reshape(4, 4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing Q-Learning for discrete action space problems\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import defaultdict\n",
        "from typing import Tuple, Callable\n",
        "\n",
        "def make_epsilon_schedule(\n",
        "    eps_start: float = 1.0,\n",
        "    eps_end: float = 0.05,\n",
        "    decay_steps: int = 20_000,\n",
        "    mode: str = \"cosine\"\n",
        ") -> Callable[[int], float]:\n",
        "    \"\"\"\n",
        "    Returns epsilon(t) in [eps_end, eps_start], decaying over 'decay_steps'.\n",
        "    Modes: 'linear' or 'cosine' (smooth).\n",
        "    \"\"\"\n",
        "    assert mode in {\"linear\", \"cosine\"}\n",
        "    def schedule(t: int) -> float:\n",
        "        x = min(t / max(1, decay_steps), 1.0)\n",
        "        if mode == \"linear\":\n",
        "            return eps_start + (eps_end - eps_start) * x\n",
        "        # cosine ease-out\n",
        "        c = (1 + np.cos(np.pi * x)) / 2  # 1→0 smoothly\n",
        "        return eps_end + (eps_start - eps_end) * c\n",
        "    return schedule\n",
        "\n",
        "def argmax_with_random_tie_breaking(q_row: np.ndarray, rng: np.random.Generator) -> int:\n",
        "    max_val = q_row.max()\n",
        "    max_idxs = np.flatnonzero(np.isclose(q_row, max_val))\n",
        "    return rng.choice(max_idxs)\n",
        "\n",
        "def epsilon_greedy_action(q_table: np.ndarray, state: int, epsilon: float,\n",
        "                          action_space_n: int, rng: np.random.Generator) -> int:\n",
        "    if rng.random() < epsilon:\n",
        "        return rng.integers(0, action_space_n)\n",
        "    return argmax_with_random_tie_breaking(q_table[state], rng)\n",
        "\n",
        "def train_q_learning(\n",
        "    env_id: str = \"FrozenLake-v1\",\n",
        "    seed: int = 7,\n",
        "    episodes: int = 50_000,\n",
        "    alpha: float = 0.8,       # learning rate\n",
        "    gamma: float = 0.99,      # discount\n",
        "    eps_start: float = 1.0,\n",
        "    eps_end: float = 0.05,\n",
        "    eps_decay_steps: int = 30_000,\n",
        "    eps_mode: str = \"cosine\",\n",
        "    render_every: int = 0,    # set >0 to watch occasionally (slow)\n",
        "    map_name: str = \"4x4\",\n",
        "    is_slippery: bool = True,\n",
        ") -> Tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Trains a Q-table on a discrete env. Returns (Q, info).\n",
        "    For FrozenLake you can pass map_name and is_slippery.\n",
        "    \"\"\"\n",
        "    # Create environment\n",
        "    if env_id == \"FrozenLake-v1\":\n",
        "        env = gym.make(env_id, map_name=map_name, is_slippery=is_slippery)\n",
        "    else:\n",
        "        env = gym.make(env_id)\n",
        "\n",
        "    # Reproducibility\n",
        "    env.reset(seed=seed)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Infer sizes\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"State must be discrete.\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"Action space must be discrete.\"\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "\n",
        "    # Initialize Q-table\n",
        "    Q = np.zeros((nS, nA), dtype=np.float32)\n",
        "\n",
        "    eps_fn = make_epsilon_schedule(eps_start, eps_end, eps_decay_steps, eps_mode)\n",
        "\n",
        "    steps = 0\n",
        "    returns = []  # episodic returns\n",
        "    lengths = []\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        state, _ = env.reset(seed=None)\n",
        "        done = False\n",
        "        ep_return = 0.0\n",
        "        ep_len = 0\n",
        "\n",
        "        while not done:\n",
        "            epsilon = eps_fn(steps)\n",
        "            action = epsilon_greedy_action(Q, state, epsilon, nA, rng)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Q-learning target\n",
        "            best_next_q = 0.0 if done else np.max(Q[next_state])\n",
        "            td_target = reward + gamma * best_next_q\n",
        "            td_error  = td_target - Q[state, action]\n",
        "            Q[state, action] += alpha * td_error\n",
        "\n",
        "            state = next_state\n",
        "            ep_return += reward\n",
        "            ep_len += 1\n",
        "            steps += 1\n",
        "\n",
        "            if render_every and (ep % render_every == 0):\n",
        "                env.render()\n",
        "\n",
        "        returns.append(ep_return)\n",
        "        lengths.append(ep_len)\n",
        "\n",
        "    env.close()\n",
        "    info = {\n",
        "        \"returns\": np.array(returns, dtype=np.float32),\n",
        "        \"lengths\": np.array(lengths, dtype=np.int32),\n",
        "        \"episodes\": episodes,\n",
        "        \"nS\": nS,\n",
        "        \"nA\": nA,\n",
        "    }\n",
        "    return Q, info\n",
        "\n",
        "def greedy_policy(Q: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns greedy action for each state.\"\"\"\n",
        "    return np.argmax(Q, axis=1)\n",
        "\n",
        "def evaluate_policy(env_id: str, policy: np.ndarray, episodes: int = 100, seed: int = 123,\n",
        "                    map_name: str = \"4x4\", is_slippery: bool = True) -> float:\n",
        "    if env_id == \"FrozenLake-v1\":\n",
        "        env = gym.make(env_id, map_name=map_name, is_slippery=is_slippery)\n",
        "    else:\n",
        "        env = gym.make(env_id)\n",
        "    env.reset(seed=seed)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    total_reward = 0.0\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = policy[s]  # deterministic greedy\n",
        "            s, r, terminated, truncated, _ = env.step(a)\n",
        "            done = terminated or truncated\n",
        "            total_reward += r\n",
        "    env.close()\n",
        "    return total_reward / episodes\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # === Train on FrozenLake (4x4). For easier learning you can set is_slippery=False.\n",
        "    Q, info = train_q_learning(\n",
        "        env_id=\"FrozenLake-v1\",\n",
        "        seed=7,\n",
        "        episodes=30_000,\n",
        "        alpha=0.8,\n",
        "        gamma=0.99,\n",
        "        eps_start=1.0,\n",
        "        eps_end=0.05,\n",
        "        eps_decay_steps=20_000,\n",
        "        eps_mode=\"cosine\",\n",
        "        map_name=\"4x4\",\n",
        "        is_slippery=True,   # try False for a much easier MDP\n",
        "    )\n",
        "\n",
        "    # Extract and evaluate greedy policy\n",
        "    pi = greedy_policy(Q)\n",
        "    avg_return = evaluate_policy(\"FrozenLake-v1\", pi, episodes=200, map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "    print(\"Q-table shape:\", Q.shape)\n",
        "    print(\"Sample of greedy policy (state -> action):\")\n",
        "    for s in range(min(16, Q.shape[0])):\n",
        "        print(f\"  {s:2d} -> {pi[s]}\")\n",
        "    print(f\"Average return over 200 eval episodes: {avg_return:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdsXkK6417su",
        "outputId": "8f606380-17e1-43d8-97ff-35c110775c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table shape: (16, 4)\n",
            "Sample of greedy policy (state -> action):\n",
            "   0 -> 0\n",
            "   1 -> 3\n",
            "   2 -> 0\n",
            "   3 -> 3\n",
            "   4 -> 0\n",
            "   5 -> 0\n",
            "   6 -> 0\n",
            "   7 -> 0\n",
            "   8 -> 3\n",
            "   9 -> 1\n",
            "  10 -> 0\n",
            "  11 -> 0\n",
            "  12 -> 0\n",
            "  13 -> 2\n",
            "  14 -> 2\n",
            "  15 -> 0\n",
            "Average return over 200 eval episodes: 0.645\n"
          ]
        }
      ]
    }
  ]
}